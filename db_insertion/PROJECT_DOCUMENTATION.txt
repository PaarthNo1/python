================================================================================
ğŸŒŠ OceanIQ Argo Data Pipeline - Technical Documentation ğŸŒŠ
================================================================================

ğŸ“… Date: 2025-11-29
ğŸ‘¤ Author: Antigravity (AI Assistant) & User
ğŸ“‚ Project: Argo Float Data Loader & Database System

--------------------------------------------------------------------------------
1. ğŸš€ Project Overview (Project Kya Hai?)
--------------------------------------------------------------------------------
Ye project **Argo Floats** (Ocean Robots) ka data automatic fetch, process aur store karne ke liye banaya gaya hai.
Argo floats samundar (ocean) mein dive karte hain aur Temperature, Salinity, Pressure, etc. measure karte hain.

**Main Goal:**
Raw NetCDF (`.nc`) files ko internet se download karna, unhein clean karna, aur ek structured **PostgreSQL Database** mein store karna taaki hum uspar queries run kar sakein aur Dashboard bana sakein.

--------------------------------------------------------------------------------
2. ğŸ—ï¸ System Architecture (Bana Kaise Hai?)
--------------------------------------------------------------------------------
Pura system 4 main parts mein divided hai:

[ ğŸŒ Argo Servers ]  --->  [ ğŸ Python Loader ]  --->  [ âš™ Parsers ]  --->  [ ğŸ—„ï¸ PostgreSQL DB ]

1.  **Data Source (Argo Servers):**
    - Hum IFREMER server (`data-argo.ifremer.fr`) se data uthate hain.
    - Humare code mein **Multi-DAC Support** hai (INCOIS, AOML, CORIOLIS, JMA, etc. sab check karta hai).

2.  **Loader (The Engine):**
    - `main_loader.py`: Ye entry point hai. Ye decide karta hai kaunse Floats process karne hain.
    - `auto_loader.py`: Ye "Brain" hai. Ye files download karta hai, duplicates check karta hai, aur parsers ko call karta hai.

3.  **Parsers (The Translators):**
    - `parsers/` folder mein scripts hain jo complex NetCDF files ko padh kar simple Python Dictionaries mein convert karti hain.
    - **Libraries:** `xarray`, `netCDF4`, `pandas`, `numpy`.

4.  **Database (The Storage):**
    - **PostgreSQL** database cloud par hosted hai (Aiven).
    - **PostGIS** extension enabled hai (Location search ke liye).
    - **Schema:**
        - `floats`: Robot ki basic info.
        - `profiles`: Har dive ki summary (Lat/Lon, Time).
        - `measurements`: Actual sensor data (Temp, Salt, etc.).
        - `traj`: Robot ka rasta (Trajectory).
        - `tech`: Technical health logs (Battery, Motor).
        - `meta_kv`: Extra metadata.

--------------------------------------------------------------------------------
3. ğŸ”„ The Pipeline (Kaam Kaise Karta Hai?)
--------------------------------------------------------------------------------
Jab aap `main_loader.py` run karte hain, to ye steps follow hote hain:

**Step 1: Discovery (Dhoondhna)**
- Code check karta hai ki Float kis DAC (Country) ka hai (e.g., India ka hai ya USA ka).
- URL automatically generate hota hai.

**Step 2: File Listing & Filtering**
- Server se saari files ki list aati hai.
- **Smart Filter:** Code database check karta hai. Agar koi Cycle pehle se database mein hai, to use **SKIP** kar diya jata hai. (Time bachata hai).

**Step 3: Download**
- Sirf naye files download hote hain.
- Files ko `dummy/data/{float_id}/` folder mein save kiya jata hai.

**Step 4: Parsing (Padhna)**
- `.nc` file open hoti hai.
- **Vectorized Operations:** `numpy` ka use karke data ko fast process kiya jata hai (Loop nahi lagaya jata).
- **Date Conversion:** Julian Days ko `pd.to_datetime` se convert kiya jata hai (Large dates aur Timezone issues handle kiye gaye hain).

**Step 5: Insertion (Save Karna)**
- Data ko Database mein bheja jata hai.
- **Batch Insert:** Ek-ek row nahi, balki 500-1000 rows ek saath bheji jati hain (`execute_batch`).

**Step 6: Optimization (Final Touch)**
- `float_summary_mv` (Materialized View) ko refresh kiya jata hai taaki Dashboard fast chale.

--------------------------------------------------------------------------------
4. ğŸ§  Key Algorithms & Logic Used
--------------------------------------------------------------------------------

**A. Multi-DAC Discovery Algorithm**
   - Humare paas ek list hai: `["incois", "aoml", "coriolis", "csiro", ...]`.
   - Code pehle `incois` check karta hai, agar 404 error aaye to `aoml` check karta hai, and so on.
   - Isse humein manually URL nahi dalna padta.

**B. Idempotency (Duplicate Prevention)**
   - Logic: `if cycle in existing_cycles_in_db: continue`
   - Ye ensure karta hai ki agar script dobara run ho, to duplicate data insert na ho.

**C. Robust Date Parsing**
   - Problem: Argo dates `Julian Days since 1950` hoti hain. Kabhi-kabhi value itni badi hoti hai ki Python crash ho jata hai.
   - Solution: Humne `pd.to_datetime(value, unit="D", origin="1950-01-01")` use kiya jo safe aur robust hai.

**D. Connection Pooling**
   - Database connection bar-bar open/close karne mein time lagta hai.
   - Hum `SQLAlchemy Connection Pool` use karte hain jo 5 connections hamesha open rakhta hai. Isse speed 10x badh gayi hai.

--------------------------------------------------------------------------------
5. ğŸ“‚ File Structure Explained
--------------------------------------------------------------------------------

root/
â”œâ”€â”€ main_loader.py          # ğŸŸ¢ Start yahan se karein. Float IDs yahan set karein.
â”œâ”€â”€ auto_loader.py          # âš™ï¸ Main logic engine. Download & Orchestration.
â”œâ”€â”€ daily_update.py         # ğŸ“… Cron job script (Daily automation ke liye).
â”œâ”€â”€ optimize_db.py          # ğŸš€ Database indexes aur views refresh karne ke liye.
â”œâ”€â”€ check_db_stats.py       # ğŸ“Š Database count check karne ke liye.
â”œâ”€â”€ dataset_cache.py        # ğŸ’¾ NetCDF files ko memory mein cache karne ke liye.
â”œâ”€â”€ .env                    # ğŸ”‘ Passwords aur DB URL yahan hain.
â”‚
â”œâ”€â”€ database/               # ğŸ—„ï¸ Database Insert Scripts
â”‚   â”œâ”€â”€ insert_profile.py
â”‚   â”œâ”€â”€ insert_measurements.py
â”‚   â”œâ”€â”€ insert_traj.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ parsers/                # ğŸ§© File Readers (NetCDF to Python)
â”‚   â”œâ”€â”€ profile.py
â”‚   â”œâ”€â”€ measurements.py
â”‚   â”œâ”€â”€ traj.py             # (Recently Fixed for Dates)
â”‚   â”œâ”€â”€ tech.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ data/                   # ğŸ“¦ Downloaded Files
    â”œâ”€â”€ 2902203/
    â”œâ”€â”€ 1902675/
    â””â”€â”€ ...

--------------------------------------------------------------------------------
6. ğŸ”® Future Scope (Aage Kya Kar Sakte Hain?)
--------------------------------------------------------------------------------
1.  **Vector Embeddings (AI Search):**
    - Hum Floats ke metadata (Location, PI Name, Sensors) ko AI Embeddings mein convert kar sakte hain.
    - Isse aap "Find floats near India with Oxygen sensor" jaisi queries natural language mein pooch sakenge.

2.  **API Development:**
    - FastAPI ka use karke ek backend bana sakte hain jo Frontend (React/Vue) ko data dega.

3.  **Real-time Dashboard:**
    - `float_summary_mv` ka use karke ek map-based dashboard banaya ja sakta hai.

--------------------------------------------------------------------------------
âœ… Documentation End
--------------------------------------------------------------------------------
